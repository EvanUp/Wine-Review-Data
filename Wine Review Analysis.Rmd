---
title: "Predicting Wine Prices with Machine Learning"
author: "Evan Williams"
date: "May 8th, 2019"
output:
  
  pdf_document: default
  html_document: default
---
```{r message=FALSE, warning=FALSE, paged.print=FALSE}
# Load libraries
library(tidyverse)
library(tidytext)
library(RCurl)
library(caret)
library(knitr)
library(kableExtra)
library(rpart)

# Pull the data from my GitHub and load it into our environment
x <- getURL("https://raw.githubusercontent.com/EvanUp/Wine-Review-Data/master/Data/winemag-data-130k-v2.csv")
wine_reviews <- read.csv(text=x, stringsAsFactors=FALSE, na.strings = "")
rm(x)
```


## Introduction

How much is a bottle of wine worth? What price should a vendor charge for a bottle given its description, area of origin, rater, and point rating? In this report, we will generate and assess several recommendation system algorithms for wine prices using [wine-reviews]("https://www.kaggle.com/zynicide/wine-reviews") dataset on Kaggle. This dataset is composed of reviews scraped from [WineEnthusiast]("https://www.winemag.com/?s=&drink_type=wine"). We will first explore the Dataset and then generate and compare several recommendation systems (fixed effects and a Random Forest) in an effort to predict price. We will evaluate these systems using Root Mean Square Error (RMSE).

## Dataset

This dataset was extracted from Kaggle and it includes reviews for over 110,000 different wines published by Wine Enthusiast Magazine between 1999 and 2017 with a rating of 80 or higher (out of 100). Wines that receive a rating below 80 are not reviewed. The full reviewing process is described [here]("https://www.winemag.com/2010/04/09/you-asked-how-is-a-wines-score-determined/"). 


To allow the data to be downloaded without a kaggle account (and by extension this code), I've uploaded the raw data to GitHub. Here is a summary of the 


Now we'll examine the dataset:
There are 129,971 rows and 14 columns
```{r}
dim(wine_reviews)
```

The dataset contains the country of origin, a description written by the reviewer, the vineyard designation within the winery, the number of points on a scale of 1-100 (that only includes reviews of 80 or more), the price in USD, the province or state of origin, the wine growing area, a more specific wine growing area, the name of taster, the taster's twitter handle, the title of the review, the grape varietal, and the winery.

```{r}
glimpse(wine_reviews)
```

Lets look at the number of distinct values for a few of these indicators

```{r}
# blanks are counted as 1, so we subtract 1 from each unique total
cat("Number of wine tasters:", (n_distinct(wine_reviews$taster_name)-1))
```

```{r}
cat("Number of countries in the dataset:", (n_distinct(wine_reviews$country)-1))
```
```{r}
cat("Number of varietals:", (n_distinct(wine_reviews$variety)-1))
```


## Data Exploration and Cleaning

In this section, we'll explore the data and clean it.

First, let's take a look at the prices, as this is the variable we are interested in predicting. We can see that the cheapest wine in the dataset is a cheap 4USD bottle, whereas the most expensive costs a whopping 3,300USD. But before we look at the data, lets answer the most important question that this dataset can answer: what are the highest rated wines for 30.00USD or less? 

P.S. - If you'd like longer version of the following tables with more detail, you can download them from my [GitHub]("https://github.com/EvanUp/Wine-Review-Data/tree/master/Data").

```{r}
summary(wine_reviews$price)
```

Here are the 10 highest rated wines for 10$ or less!

```{r message=FALSE, warning=FALSE, paged.print=FALSE}
wine_reviews %>% filter(price <=10) %>%  select(title, price, points) %>% arrange(desc(points)) %>% slice(1:10) %>% rename(Wine=`title`, Price_USD =`price`, Points = `points`) %>% 
  kable("html", align = "c", caption = "Best Wines under 10$") %>% 
  kable_styling(bootstrap_options = 
                  c("striped", "condensed", "bordered"), 
                  full_width = FALSE)
```

Here are the 10 highest rated wines 20$ or less!

```{r message=FALSE, warning=FALSE, paged.print=FALSE}
wine_reviews %>% filter(price <=20) %>% 
arrange(desc(points)) %>% select(title, price, points) %>% slice(1:10) %>% rename(Wine=`title`, Price_USD =`price`, Points = `points`) %>%
  kable("html", align = "c", caption = "Best Wines under 20$") %>% 
  kable_styling(bootstrap_options = 
                  c("striped", "condensed", "bordered"), 
                  full_width = FALSE)
```

And finally the 10 highest rated wines for 30$ or less!

```{r message=FALSE, warning=FALSE, paged.print=FALSE}
wine_reviews %>% filter(price <=30) %>% 
  arrange(desc(points)) %>% select(title, price, points) %>% slice(1:10) %>% rename(Wine=`title`, Price_USD =`price`, Points = `points`) %>% 
  kable("html", align = "c", caption = "Best Wines under 30$") %>% 
  kable_styling(bootstrap_options = 
                  c("striped", "condensed", "bordered"), 
                  full_width = FALSE)
```

Now that the most important question is out of the way, lets look more in depth at the distributions of points and price as well as their relationship. 

**Points**

Points are the ratings that Sommeliers gave the wines. We know that the range is between 80 and 100, but lets look at the distribution of point scores.

We can see that the majority of the ratings fall in the upper 80s (around 89). The data looks like a more or less normal distribution.

```{r message=FALSE, warning=FALSE, paged.print=FALSE}
wine_reviews %>%
  ggplot(aes(points)) +
  geom_histogram(bins=20, fill="coral4", color="red4") +
  labs(title="Distribution of points")+
  theme(plot.title = element_text(hjust = 0.5))
```

**Price**

We can see that about 99% of wines that were rated were $150 or less- additionally we see farther down a wide confidence interval for wines over that price. We also see there are 8996 wines with missing prices.

```{r warning=FALSE}
wine_reviews %>% ggplot(aes(x=price)) + 
  geom_histogram(bins=25, fill="coral4", color="red4") + 
  scale_x_log10() +
  labs(x = "price (log10 scale)", title= "Distribution of Prices")+
  theme(plot.title = element_text(hjust = 0.5))
```

```{r}
cat("Percentage of wines over 100USD: ",mean(wine_reviews$price>150, na.rm=TRUE)*100,"%", sep="")
```
```{r}
cat("Number of wines with no price data",  sum(is.na(wine_reviews$price)))
```


When we plot points against price, there does seem to be a relationship trend, but that trend has higher variability when it reaches bottles worth over 150USD (only about 1% of the data). To improve our algorithm's accuracy, we can trim price at 150USD. If a vendor thinks their wine is worth over 150USD, they'd benefit from consulting sommeliers directly. We can also drop wines with no price data as we're interested in the relationship between price and quality.

```{r message=FALSE}
wine_reviews %>% filter(price != is.na(price)) %>% 
  ggplot(aes(x=price,y=points))+
  geom_hex()+
  geom_smooth(color="red4", span=.2)+
  scale_x_log10()+
  labs(title="Price distribution by points level", y="Points", x="Price (log10)")+
  theme(plot.title = element_text(hjust = 0.5))
```

We'll trim the data and drop all wines over 200USD.
```{r}
# Filter silently drops NA's
wr_150 <- wine_reviews %>% filter(price <= 150)
```


When we trim price, we can see that there's a rapid increase in points up until about 30$ per bottle, but after that, points increase more gradually.
```{r message=FALSE}
wr_150 %>%
  ggplot(aes(x=price,y=points))+
  geom_hex()+
  geom_smooth(color="red4", span=.2)+
  labs(title="Price distribution by points level", y="Points", x="Price")+
  theme(plot.title = element_text(hjust = 0.5))
```


**Country**

Lets examine the top 10 countries of origin of the wines reviewed. We can see that the majority of the wine reviews were of US wines (with over 50,000 wines reviewed). France is the country with the second highest number of wines reviewed (over 20,000 French wines were reviewed, but we dropped many of them when we trimmed and filtered price.)

```{r}
wr_150 %>% group_by(country)%>% summarize(count = n()) %>% arrange(desc(count)) %>% slice(1:10) %>% ggplot(aes(x =reorder(country, count), y =  count )) +
  geom_bar(stat='identity',colour="red4", fill = "coral4") +
  theme(plot.title = element_text(hjust = 0.5))+
  labs(x = '', y = "Number of Wines Rated", title = 'Total wines rated by country (top 10 countries)') +
  ylim(0, 60000)+
  coord_flip()
```


**Province**

We can drill down further and look at the province (or state) of origin for the wines. We know the data are dominated by US wines, so we should expect to see California as number one. However, we also see a decent number of wines from Tuscany, Bourdeaux, Northern Spain, and a few other foreign provinces.

```{r}
wr_150 %>% group_by(province)%>% summarize(count = n()) %>% arrange(desc(count)) %>% slice(1:10) %>% ggplot(aes(x =reorder(province, count), y =  count )) +
  geom_bar(stat='identity',colour="red4", fill = "coral4") +
  theme(plot.title = element_text(hjust = 0.5))+
  labs(x = '', y = "Number of Wines Rated", title = 'Total wines rated by province (top 10 provinces)') +
  ylim(0, 40000)+
  coord_flip()
```

Additionally, we can see that there are 59 missing provinces. We can drop these as we'll use province in our algorithm- anyone wanting to know what a wine is worth would surely know the province or state that it was made in.

```{r}
cat("Data with missing province:", sum(is.na(wr_150$province)))
```

```{r}
wr_150_p <- wr_150 %>% drop_na(province)
```


**Taster**

Finally, before we dive into the text analysis, lets take a look at the tasters. The majority of wines were rated by anonymous tasters, but Roger Voss rated nearly 20,000 wines under the price of 150$. Roger Voss often rates the most expensive French wines, so his real total is much higher. I have a new personal hero.

```{r}
wr_150_p %>% group_by(taster_name)%>% summarize(count = n()) %>% arrange(desc(count)) %>% slice(1:10) %>% ggplot(aes(x =reorder(taster_name, count), y =  count )) +
  geom_bar(stat='identity',colour="red4", fill = "coral4") +
  theme(plot.title = element_text(hjust = 0.5))+
  labs(x = '', y = "Number of Wines Rated", title = 'Total wines rated by taster (top 10 tasters)') +
  ylim(0, 25000)+
  coord_flip()
```

**Description**

Given that descriptions are written descriptions of wine, it's strange that there would be duplicate descriptions- but we see that there are over 9,000 repeated wine descriptions in our dataset. If we want to use text analysis, we'll need to drop these duplicates.
```{r}
cat("Number of Duplicated Descriptions:", nrow(wr_150_p) - n_distinct(wr_150_p$description))
```

We'll create our final, clean dataset by removing these duplicates.

```{r}
wr_clean <- wr_150_p %>% 
  mutate(dups = duplicated(description)) %>% 
  filter(dups==FALSE) %>% 
  select(-dups)
```



## Natural Language Processing and Cleaning

In this section, we will examine the most-used words in descsriptions of wine and we'll create a variable for length of descriptions. We'll also do a small amount of cleaning.


```{r warning=FALSE, message=FALSE}
wine_descriptions <- wr_clean %>%
  mutate(description = tolower(description)) %>% 
  unnest_tokens(word, description) %>%
  anti_join(stop_words) %>%
  distinct()
```

Unsurprisingly, the most-used words are the words we associate with wine: flavors, wine, fruit, aromas, palate, finish, etc.

```{r paged.print=FALSE, message=FALSE}
wine_descriptions %>%
  count(word, sort = TRUE) %>%
  top_n(10) %>%
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_bar(stat='identity',colour="red4", fill = "coral4") +
  theme(plot.title = element_text(hjust = 0.5))+
  labs(x = '', y = "Word Count", title = 'Most Frequently Used Words') +
  coord_flip()
```


Now lets look at wordcount per review- perhaps there's a relationship between the length of a review and the quality of a wine.

```{r}
wine_des_total <- wr_clean %>%
  mutate(description = tolower(description)) %>% 
  unnest_tokens(word, description) %>%
  group_by(X, price) %>% 
  summarize(num_words = n()) %>% 
  arrange(desc(num_words))
```

Like price, there seems to be somewhat of a normal distribution of wordcounts.

```{r}
wine_des_total %>% ggplot(aes(x=num_words)) + 
  geom_histogram(bins=20, fill="coral4", color="red4") + 
  scale_x_log10() +
  labs(x = "number of words", y= "count", title= "Distribution of wordcount")+
  theme(plot.title = element_text(hjust = 0.5))
```

When we look at price against wordcount, we see somewhat of a relationship. More words seem to generally indicate better reviews, but we see a wider range around low prices.

```{r message=FALSE}
wine_des_total %>%
  ggplot(aes(x=price,y=num_words))+
  geom_hex()+
  geom_smooth(color="red4", span=.2)+
  labs(title="Price", y="number of words", x="Price")+
  theme(plot.title = element_text(hjust = 0.5))
```
We can drop descriptions over 100 words and below 10 words as these are clearly outliers and we can merge this into our dataframe. This will drop 68 observations. Finally, we'll convert characters to factors.

```{r}
wine_des_total1 <- wine_des_total %>% select(-price) %>% 
  filter(num_words>=10 & num_words<=100)

wr_clean_num <- wr_clean %>% left_join(wine_des_total1, by="X") %>% 
  drop_na(num_words) %>% 
  mutate(province = factor(province),
         variety = factor(variety),
         taster_name = factor(taster_name),
         winery = factor(winery))
```


## Methods

To create a price recommendation system, we'll implement the model-based approach that we learned in the machine learning course on Edx. We'll first consider a model where we simply recommend the same price for all wines (by taking average price), and then we'll add controls for  province effects, points effects, taster effects, and wordcount effects in order to lower the RMSE. 

Ideally, these effects would be considered using regressions, but unfortunately, attempting to do so would likely crash this computer. Rather, we'll compute an approximation by estimating the overall mean, `mu`. We will then use this to find the province effects, points effects, taster effects, and wordcount effects. Each of these controls is added to our approximation of a regression line, and each subsequent feature will be added to our approximation of a regression line sequentially.

The predicted results for each model wil thus be:  
Simple Average: `mu`  
Province Effects: `mu` + `province effects`  
Points Effects: `mu` + `province effects` + `points_effects`  
Taster Effects: `mu` + `province effects` + `points_effects` + `taster effects`  
Wordcount Effects: `mu` + `province effects` + `points_effects` + `taster effects` + `wordcount`

Next, we will run a random forest over the data and compare the results to the fixed effects models. Random forests generate Classification and Regression Trees which stratify data based on if-else rules. The rules divide the dataset into distinct and non-overlapping regions (recursive binary splitting). Random forests generate several trees and obtain a final prediction by averaging or voting.

The models will be evaluated using the root-mean-square error (RMSE). The RMSE is simply the standard deviation of the residuals. The formula is below.

```{r}
RMSE <- function(true_prices, predicted_prices){
    sqrt(mean((true_prices - predicted_prices)^2))
}
```

## Results

Before anything else, we'll partition the data into a training set and a test set. The training set contains 90% of the data and the testing set contains 10% of the data.

```{r message=FALSE}
set.seed(123)
test_index <- createDataPartition(y = wr_clean_num$price, times = 1, p = 0.10, list = FALSE)
train_set <- wr_clean_num[-test_index,]
temporary_set <- wr_clean_num[test_index,]
# Making sure the variables we care about are in both datasets.
test_set <- temporary_set %>%
      semi_join(train_set, by = "country") %>%
      semi_join(train_set, by = "province") %>%
      semi_join(train_set, by = "taster_name") %>%
      semi_join(train_set, by = "variety") %>%
      semi_join(train_set, by = "winery") %>%
      semi_join(train_set, by = "num_words")
removed_rows <- anti_join(temporary_set, test_set)
train_set <- rbind(train_set, removed_rows)
rm(removed_rows, temporary_set, test_index)
```

First, lets try simply filling all missings with the average and seeing how well that model does at pricing

```{r}
mu <- mean(train_set$price)
naive_rmse <- RMSE(test_set$price, mu)
rmse_results <- bind_rows(tibble(Method = "Just the average", RMSE = naive_rmse))
kable(rmse_results,align=rep("c",3)) %>%
  kable_styling(full_width = F) %>%
  column_spec(1,bold=T,border_right = T)
```

For a producer, overcharging or undercharging by an average 22.90USD wouldn't be ideal as the producer would likely either be unable to sell his or her wine, or sell far below profit. We'll try adding in fixed effects in order to bring down the RMSE. Simply adding province brings our error down to 21.16USD. Lets add a few more effects and see how it changes RMSE.

```{r}
prov_avgs <- train_set %>%
  group_by(province) %>%
  summarize(province_effect = mean(price - mu))

predicted_ratings <- mu + test_set %>%
  left_join(prov_avgs, by="province") %>%
  .$province_effect

province_effect_RMSE <- RMSE(predicted_ratings, test_set$price)
rmse_results <- bind_rows(rmse_results,
                          data_frame(Method= "With Province Effect",
                                     RMSE=province_effect_RMSE))

kable(rmse_results,align=rep("c",3)) %>%
  kable_styling(full_width = F) %>%
  column_spec(1,bold=T,border_right = T)
```

Next lets add Points Effects
```{r}

points_avgs <- train_set %>%
  left_join(prov_avgs, by='province') %>%
  group_by(points) %>%
  summarize(points_effect = mean(price - mu - province_effect))

predicted_ratings <- test_set %>%
  left_join(prov_avgs, by="province") %>%
  left_join(points_avgs, by='points') %>%
  mutate(pred = mu + province_effect + points_effect) %>% 
  .$pred

points_effect_RMSE <- RMSE(predicted_ratings, test_set$price)
rmse_results <- bind_rows(rmse_results,
                          data_frame(Method= "With Points Effect",
                                     RMSE=points_effect_RMSE))

kable(rmse_results,align=rep("c",3)) %>%
  kable_styling(full_width = F) %>%
  column_spec(1,bold=T,border_right = T)
```

Next lets add Taster Effects

```{r message=FALSE, warning=FALSE}

taster_avgs <- train_set %>%
  left_join(prov_avgs, by='province') %>%
  left_join(points_avgs, by='points') %>%
  group_by(taster_name) %>%
  summarize(taster_effect = mean(price - mu -province_effect-points_effect))

predicted_ratings <- test_set %>%
  left_join(prov_avgs, by="province") %>%
  left_join(points_avgs, by='points') %>%
  left_join(taster_avgs, by='taster_name') %>%
  mutate(pred = mu + province_effect + points_effect+taster_effect) %>% 
  .$pred

taster_effect_RMSE <- RMSE(predicted_ratings, test_set$price)
rmse_results <- bind_rows(rmse_results,
                          data_frame(Method= "With Taster Effect",
                                     RMSE=taster_effect_RMSE))

kable(rmse_results,align=rep("c",3)) %>%
  kable_styling(full_width = F) %>%
  column_spec(1,bold=T,border_right = T)
```

Next we'll add Wordcount Effects

```{r message=FALSE, warning=FALSE}

wc_avgs <- train_set %>%
  left_join(prov_avgs, by='province') %>%
  left_join(points_avgs, by='points') %>%
  left_join(taster_avgs, by='taster_name') %>%
  group_by(num_words) %>%
  summarize(wordcount_effect = mean(price - mu -province_effect-points_effect-taster_effect))

predicted_ratings <- test_set %>%
  left_join(prov_avgs, by="province") %>%
  left_join(points_avgs, by='points') %>%
  left_join(taster_avgs, by='taster_name') %>%
  left_join(wc_avgs, by='num_words') %>%
  mutate(pred = mu + province_effect + points_effect + taster_effect + wordcount_effect) %>% 
  .$pred

wordcount_effect_RMSE <- RMSE(predicted_ratings, test_set$price)
rmse_results <- bind_rows(rmse_results,
                          data_frame(Method= "With Wordcount Effect",
                                     RMSE=wordcount_effect_RMSE))

kable(rmse_results,align=rep("c",3)) %>%
  kable_styling(full_width = F) %>%
  column_spec(1,bold=T,border_right = T)
```

Finally, we'll run a random forest using all of the same features as our wordcount effect model:

```{r}
tree <- rpart(price ~ province+points+taster_name+num_words, data = train_set)
pred <- predict(tree, test_set)
RFRMSE <-RMSE(test_set$price, pred)

rmse_results <- bind_rows(rmse_results,
                          data_frame(Method= "Random Forest",
                                     RMSE=RFRMSE))

kable(rmse_results,align=rep("c",3)) %>%
  kable_styling(full_width = F) %>%
  column_spec(1,bold=T,border_right = T)
```

## Conclusion

In conclusion, the fixed Effects model actually outperformed a Random Forest model that was using the same data. That being said, these simple models reduced the RMSE by 23% from the simple average. We now have a model that can generally price a wine within 17.63USD of its correct value.

If this model were to be developed further, we could add more NLP variables, and we could incorporate wineries, grape varieties, and years into the model. Additionally, it would be a good idea to try other machine learning methods to see if we can reduce the RMSE even further.